# Define the MSE functions
mse_p1 <- function(p, n) {
return((p * (1 - p)) / n)
}
mse_p2 <- function(p, n) {
return((1 / (n + 2)^2) * (n * p * (1 - p) + (1 - 2 * p)^2))
}
# Set up a sequence of p values
p_values <- seq(0, 1, length.out = 500)
# Set a sample size n
n <- 100
# Calculate MSE for both estimators across p values
mse_1_values <- sapply(p_values, mse_p1, n = n)
mse_2_values <- sapply(p_values, mse_p2, n = n)
# Plot the MSEs for both estimators
plot(p_values, mse_1_values, type = "l", col = "blue", lwd = 2,
ylab = "Mean Squared Error (MSE)", xlab = "p",
main = paste("MSE of Estimators p1 and p2 for n =", n))
lines(p_values, mse_2_values, col = "red", lwd = 2)
legend("topright", legend = c("MSE(p1) = Y/n", "MSE(p2) = (Y + 1)/(n + 2)"),
col = c("blue", "red"), lty = 1, lwd = 2)
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores() - 1)  # Use one less core than available
registerDoParallel(cl)
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
# Convert ACTION to factor in train dataset
train$ACTION <- as.factor(train$ACTION)
library(tidymodels)
library(embed)
library(vroom)
library(reshape2)
library(lme4)
library(kknn)
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores() - 1)  # Use one less core than available
registerDoParallel(cl)
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
# Convert ACTION to factor in train dataset
train$ACTION <- as.factor(train$ACTION)
# Set up the recipe
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))  # Target encoding
# Set up the recipe
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))  # Target encoding
# Prepare the recipe
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
# Specify the random forest model with tunable parameters
my_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 200) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create a workflow
randForest_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
# Set up cross-validation (5-fold CV)
cv_splits <- vfold_cv(train, v = 5)
# Create a grid of tuning parameters
param_grid <- grid_regular(
mtry(range = c(1, 5)),  # Reduced range for mtry
min_n(range = c(2, 10)),  # Set specific range for min_n
levels = 3               # Reduce number of levels
)
# Tune the random forest model with cross-validation
tuned_results <- tune_grid(
randForest_wf,
resamples = cv_splits,
grid = param_grid,
metrics = metric_set(roc_auc),
control = control_grid(verbose = TRUE)
)
stopCluster(cl)
# Extract the best parameters based on ROC AUC
best_params <- select_best(tuned_results, "roc_auc")
# Finalize the workflow with the best parameters
final_wf <- finalize_workflow(
randForest_wf,
best_params
)
# Extract the best parameters based on ROC AUC
best_params <- select_best(tuned_results, metric = "roc_auc")
# Finalize the workflow with the best parameters
final_wf <- finalize_workflow(
randForest_wf,
best_params
)
# Fit the final workflow on the entire training data
final_fit <- fit(final_wf, data = train)
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test)
stopCluster(cl)
stopCluster(cl)
library(tidymodels)
library(embed)
library(vroom)
library(reshape2)
library(lme4)
library(kknn)
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
# Convert ACTION to factor in train dataset
train$ACTION <- as.factor(train$ACTION)
# Set up the recipe
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))  # Target encoding
# Prepare the recipe
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
# Specify the random forest model with tunable parameters
my_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 200) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create a workflow
randForest_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
# Set up cross-validation (5-fold CV)
cv_splits <- vfold_cv(train, v = 5)
# Create a grid of tuning parameters
param_grid <- grid_regular(
mtry(range = c(1, 5)),  # Reduced range for mtry
min_n(range = c(2, 10)),  # Set specific range for min_n
levels = 3               # Reduce number of levels
)
# Tune the random forest model with cross-validation
tuned_results <- tune_grid(
randForest_wf,
resamples = cv_splits,
grid = param_grid,
metrics = metric_set(roc_auc),
control = control_grid(verbose = TRUE)
)
stopCluster(cl)
stopCluster()
library(tidymodels)
library(embed)
library(vroom)
library(reshape2)
library(lme4)
library(kknn)
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
# Convert ACTION to factor in train dataset
train$ACTION <- as.factor(train$ACTION)
# Set up the recipe
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))  # Target encoding
# Prepare the recipe
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
# Specify the random forest model with tunable parameters
my_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 200) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create a workflow
randForest_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
# Set up cross-validation (5-fold CV)
cv_splits <- vfold_cv(train, v = 5)
# Create a grid of tuning parameters
param_grid <- grid_regular(
mtry(range = c(1, 5)),  # Reduced range for mtry
min_n(range = c(2, 10)),  # Set specific range for min_n
levels = 3               # Reduce number of levels
)
# Tune the random forest model with cross-validation
tuned_results <- tune_grid(
randForest_wf,
resamples = cv_splits,
grid = param_grid,
metrics = metric_set(roc_auc),
control = control_grid(verbose = TRUE)
)
# Extract the best parameters based on ROC AUC
best_params <- select_best(tuned_results, metric = "roc_auc")
# Finalize the workflow with the best parameters
final_wf <- finalize_workflow(
randForest_wf,
best_params
)
# Fit the final workflow on the entire training data
final_fit <- fit(final_wf, data = train)
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test)
submission <- test %>%
dplyr::select(id) %>%
mutate(ACTION = test_predictions$.pred_1)
# Save the submission file
vroom_write(submission,
path = "/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/Rand_ForestBATCH.csv",
delim = ",")
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test, type = "prob")
submission <- test %>%
dplyr::select(id) %>%
mutate(ACTION = test_predictions$.pred_1)
# Save the submission file
vroom_write(submission,
path = "/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/Rand_ForestBATCH.csv",
delim = ",")
# Load necessary libraries
library(dplyr)
# Load necessary libraries
library(dplyr)
# Load the dataset (replace the path with your actual file location)
nba_data <- read.csv("/Users/carsoncollins/Desktop/Stats 234/archive/NBA_2024_per_game(26-01-2024).csv")
# Step 1: Identify unique teams
unique_teams <- unique(nba_data$Tm)
# Step 2: Perform cluster sampling (select 5 random teams as clusters)
set.seed(42)  # For reproducibility
sampled_teams <- sample(unique_teams, size = 5, replace = FALSE)
# Step 3: Filter the dataset for the sampled teams
cluster_sample <- nba_data %>% filter(Tm %in% sampled_teams)
# Step 4: Compute the point estimate (average points per game for the sampled teams)
point_estimate <- mean(cluster_sample$PTS, na.rm = TRUE)
# Step 5: Calculate the 95% confidence interval
n <- nrow(cluster_sample)
std_error <- sd(cluster_sample$PTS, na.rm = TRUE) / sqrt(n)
# Use the t-distribution to calculate the confidence interval
alpha <- 0.05
t_value <- qt(1 - alpha / 2, df = n - 1)
ci_lower <- point_estimate - t_value * std_error
ci_upper <- point_estimate + t_value * std_error
# Output results
cat("Point Estimate (Average PTS):", point_estimate, "\n")
cat("95% Confidence Interval:", ci_lower, "-", ci_upper, "\n")
ssh ccarsonj@becker.byu.edu
ssh ccarsonj@becker.byu.edu
qnorm(.025,.975)
qnorm(.025)
qnorm(.975)
qnorm(.05)
qnorm(.95)
qchisq(.025)
qchisq(.025)
qchisq(1,.025)
qchisq(.025,1)
qchisq(.975,1)
qchisq(.025)
qchisq(.025,1)
qchisq(.975,1)
qchisq(.05)
qchisq(.05,1)
qchisq(.95,1)
install.packages("themis")
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
library(tidymodels)
library(embed)
library(vroom)
library(reshape2)
library(lme4)
library(kknn)
library(themis)
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
# Convert ACTION to factor in train dataset
train$ACTION <- as.factor(train$ACTION)
# Set up the recipe
my_recipe <- recipe(ACTION ~ ., data=train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_dummy(all_predictors()) %>%
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold = 0.9) %>%
step_smote(all_outcomes(), neighbors=4)
prepped_recipe <- prep(my_recipe)
baked <- bake(prep, new_data = train)
# Specify the random forest model with tunable parameters
my_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = tune()) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create a workflow
randForest_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
# Set up cross-validation (5-fold CV)
cv_splits <- vfold_cv(train, v = 5)
# Create a grid of tuning parameters
param_grid <- grid_regular(
mtry(range = c(1, 5)),  # Reduced range for mtry
min_n(range = c(2, 10)),
trees(range = c(300,1000)), # Set specific range for min_n
levels = 3               # Reduce number of levels
)
# Tune the random forest model with cross-validation
tuned_results <- tune_grid(
randForest_wf,
resamples = cv_splits,
grid = param_grid,
metrics = metric_set(roc_auc),
control = control_grid(verbose = TRUE)
)
View(predictions)
View(param_grid)
# Extract the best parameters based on ROC AUC
best_params <- select_best(tuned_results, metric = "roc_auc")
# Finalize the workflow with the best parameters
final_wf <- finalize_workflow(
randForest_wf,
best_params
)
# Fit the final workflow on the entire training data
final_fit <- fit(final_wf, data = train)
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test, type = "prob")
submission <- test %>%
dplyr::select(id) %>%
mutate(ACTION = test_predictions$.pred_1)
# Save the submission file
vroom_write(submission,
path = "/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/Rand_ForestTUNE.csv",
delim = ",")
library(tidyverse)
library(tidymodels)
library(embed)
library(vroom)
library(workflows)
library(glmnet)
library(ranger)
library(doParallel)
cl <- makePSOCKcluster(8)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
setwd("~/Desktop/Stats348/AmazonEmployeeAcess")
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>% mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION~., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
rf_model <- rand_forest(mtry=tune(),
min_n=tune(),
trees=500) %>%
set_mode("classification") %>%
set_engine("ranger")
rf_workflow <- workflow() %>%
add_model(rf_model) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(mtry(range=c(1, 9)), min_n(), levels = 3)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- rf_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best_tune <- cv_results %>% select_best(metric='roc_auc')
final_workflow <- rf_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = train)
rf_preds <- predict(final_workflow,
new_data = test,
type = 'prob')
rf_submission <- rf_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=rf_submission, file="./Rand_ForestNEW.csv", delim=",")
stopCluster(cl)
library(tidyverse)
library(tidymodels)
library(embed)
library(vroom)
library(workflows)
library(glmnet)
library(ranger)
library(doParallel)
cl <- makePSOCKcluster(8)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>% mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION~., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
rf_model <- rand_forest(mtry=tune(),
min_n=tune(),
trees=1000) %>%
set_mode("classification") %>%
set_engine("ranger")
rf_workflow <- workflow() %>%
add_model(rf_model) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(mtry(range=c(1, 9)), min_n(), levels = 3)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- rf_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best_tune <- cv_results %>% select_best(metric='roc_auc')
final_workflow <- rf_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = train)
rf_preds <- predict(final_workflow,
new_data = test,
type = 'prob')
rf_submission <- rf_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=rf_submission, file="./Rand_ForestNEW.csv", delim=",")
stopCluster(cl)
setwd("~/Desktop/Stats348/AmazonEmployeeAcess")
library(tidyverse)
library(tidymodels)
library(embed)
library(vroom)
library(workflows)
library(glmnet)
library(ranger)
library(doParallel)
cl <- makePSOCKcluster(8)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>% mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION~., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
rf_model <- rand_forest(mtry=tune(),
min_n=tune(),
trees=1000) %>%
set_mode("classification") %>%
set_engine("ranger")
rf_workflow <- workflow() %>%
add_model(rf_model) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(mtry(range=c(1, 9)), min_n(), levels = 1)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- rf_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best_tune <- cv_results %>% select_best(metric='roc_auc')
final_workflow <- rf_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = train)
rf_preds <- predict(final_workflow,
new_data = test,
type = 'prob')
rf_submission <- rf_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=rf_submission, file="./Rand_ForestNEW.csv", delim=",")
stopCluster(cl)
setwd("~/Desktop/Stats348/Ghouls-Goblins-and-Ghosts")
library(tidymodels)
library(tidyverse)
library(vroom)
train <- vroom("./trainWithMissingValues.csv")
test <- vroom("./test.csv")
View(test)
View(train)
my_recipe <- recipe(TYPE~., data = train) %>%
step_impute_mean(all_numeric_predictors())
my_recipe <- recipe(type~., data = train) %>%
step_impute_mean(all_numeric_predictors())
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
View(baked)
View(baked)
MissingTrain <- vroom("./trainWithMissingValues.csv")
train <- vroom("./train.csv")
test <- vroom("./test.csv")
my_recipe <- recipe(type~., data = train) %>%
step_impute_mean(all_numeric_predictors())
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
my_recipe <- recipe(type~., data = Missingtrain) %>%
step_impute_mean(all_numeric_predictors())
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
MissingTrain <- vroom("./trainWithMissingValues.csv")
train <- vroom("./train.csv")
test <- vroom("./test.csv")
my_recipe <- recipe(type~., data = MissingTrain) %>%
step_impute_mean(all_numeric_predictors())
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
library(tidymodels)
library(tidyverse)
library(vroom)
MissingTrain <- vroom("./trainWithMissingValues.csv")
MissingTrain <- vroom("./trainWithMissingValues.csv")
train <- vroom("./train.csv")
test <- vroom("./test.csv")
my_recipe <- recipe(type~., data = MissingTrain) %>%
step_impute_mean(all_numeric_predictors())
prep <- prep(my_recipe)
baked <- bake(prep, new_data = MissingTrain)
rmse_vec(train[is.na(MissingTrain)], baked[is.na(MissingTrain)])
my_recipe <- recipe(type~., data = MissingTrain) %>%
step_impute_bag(all_numeric_predictors(), impute_with = imp_vars(all_predictors()))
prep <- prep(my_recipe)
baked <- bake(prep, new_data = MissingTrain)
rmse_vec(train[is.na(MissingTrain)], baked[is.na(MissingTrain)])
